---
title: "portalcasting"
output: rmarkdown::html_vignette
author: "Juniper L. Simonis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
vignette: >
  %\VignetteIndexEntry{codebase}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include=FALSE}
library(portalcasting)
vers <- packageVersion("portalcasting")
today <- Sys.Date()
```

This vignette outlines the codebase and functionality of the **portalcasting** package (v`r vers`), which underlies the automated iterative forecasting within the [Portal Predictions production pipeline](https://github.com/weecology/portalPredictions). **portalcasting** has utilities for setting up local versions of the pipeline for developing and testing new models, which are covered in detail in other vignettes.

## Installation

To install the most recent version of **portalcasting** from GitHub:

```{r, eval=FALSE}
install.packages("devtools")
devtools::install_github("weecology/portalcasting")
```

## Directory Structure

The package uses a directory tree with two levels to organize the project:

* `main`: project folder encompassing all subfolders
* `subs`: specific subfolders that organize the project files
 
structured as

```
main
│
└──predictions
│   <previous and current predictions>
└──models
│   <model scripts>
└──data
│   <data used for a specific run of models>
└──raw
│   <stable version of raw components used to populate other folders>
└──tmp
│   <temporary files>
```

(**Note**: Previous versions of **portalcasting** had an explicitly named `base` level more basal than `main`, but that has been removed as of v0.9.0. To group the project subfolders into a multi-leveled folder, simply add structure to the `main` input, such as `main = "~/project_folder"`.)

## Instantiating a Directory

Setting up a fully functional directory for a production or sandbox pipeline consists of two steps: creating (instantiating folders that are missing) and filling (adding files to the folders). These steps can be executed separately or in combination via a general `setup_dir` function or via specialized versions of `setup_dir`: `setup_sandbox` (for creating a pipeline with defaults to facilitate sandboxing) and `setup_production` (for creating a production pipeline). These functions are general and flexible, allowing for customization through a variety of arguments, but are designed to work well under default settings.

### Creating 

The directory is established using `create_dir`, which takes `main` and `subs` as arguments and in sequence creates each of the levels' folders if they do not already exist. This occurs through calls to `create_main` and `create_subs`. A typical user is likely to want to change the `main` input (to locate the forecasting directory where they would like it), but general users should not need to alter the input to `subs`.

### Filling 

The directory is filled (loaded with files for default forecasting) using a series of subdirectory-specific functions that are combined in the overall `fill_dir` function:

* `fill_raw` downloads each of the raw components for the directory, such as the source data and previous forecasts.
* `fill_predictions` moves the existing predictions files from the `raw` subdirectory to the `predictions` subdirectory.
* `fill_models` writes the model scripts into the `models` subdirectory.
* `fill_data` prepares the forecasting data files from the raw downloaded data files and moves them into the `data` subdirectory.
  * `prep_moons` prepares and formats the temporal (lunar) data from the raw data.
  * `prep_rodents` prepares multiple structures of the rodents data for analyses from the raw data.
  * `prep_covariates` downloads and forecasts covariates data.
  * `prep_metadata` creates and saves out a YAML metadata list for the forecasting. 

Each of these components can be run individually, as well.

## Running models

Models are run using a function pipeline similar to the creation and filling function pipelines, with flexible controls through a variety of arguments, but robust operation under default settings. 

* `portalcast` is the overarching function that controls -casting of the Portal data
  * `verify_models` ensures that all of the models requested have scripts present in the `models` subdirectory.
  * `verify_raw_data` ensures that the base Portal data are present in the `raw` subdirectory
  * `extract_min_lag` determines the minimum non-0 lag used with covariate data across the models, for input into `prep_data`
  * `read_moons` brings the lunar data in to the function and `last_newmoon` determines the most recently passed newmoon, which is used to set the forecast origin (`end_moons`, note that here the plural in `end_moons` indicates that multiple forecast origins can be input to `portalcast`) if it wasn't set by the user.
  * For each `end_moons` value, `portalcast` runs:
    * `prep_data` which ensures that the data files in the `data` subdirectory are up-to-date for the specifics requested. 
      * The metadata file is read in and the dates and moons are checked for being up to date.
      * If the metadata file doesn't exist, the date of the data is old, the newmoons aren't aligned, or a hindcast is being done (the `end_moon` is lower than the `last_moon`), the data are updated using `fill_data`.
    * `cast` runs ("casts") each of the requested models for the data
      * `clear_tmp` clears the files in the `tmp` folder.
      * `models_to_cast` collects the file paths to the scripts in the `models` subdirectory, which are then run using `source`.
      * `combine_casts` collects the individual model casts.
      * `add_ensemble` adds a simple AIC-based ensemble to the output. 
      * `clear_tmp` clears the files in the `tmp` folder.


## Figures

There are currently four main figure functions, each of which involves some level of data and forecast processing. The default settings for each function are designed to produce the figures on the [Portal Predictions site](http://portal.naturecast.org/), but are flexible to allow for a wider range of options. 

* `plot_cast_ts` makes the timeseries plot:
  * `most_recent_cast_date` determines the date of the most recent -cast
  * `read_rodents_table` reads in observations.
  * `read_casts` reads in the casts.
  * `select_casts` chooses among the read-in casts.
  * `plot_cast_ts_xaxis`
  * `plot_cast_ts_ylab`
* `plot_cast_point` makes the point-in-time distribution plots (with or without observed data):
  * `most_recent_cast_date` determines the date of the most recent -cast
  * `most_recent_census` finds the most recent observation (rodent census) in the data.
  * `read_rodents_table` reads in observations.
  * `read_casts` reads in the casts.
  * `select_casts` chooses among the read-in casts.
  * `plot_cast_point_yaxis`
* `plot_err_lead_spp_mods` shows the err as a function of lead time for multiple models across species:
  * `read_casts` reads in the casts.
  * `select_casts` chooses among the read-in casts.
  * `append_observed_to_cast` combines the observed data to the forecasted data.
* `plot_cov_RMSE_mod_spp` displays the Root Mean Squared Error (RMSE) and coverage (cov) as a funciton of model for each species:
  * `read_casts` reads in the casts.
  * `select_casts` chooses among the read-in casts.
  * `append_observed_to_cast` combines the observed data to the forecasted data.
  * `measure_cast_error` calculates error values, given the observations and casts.




## Utilities

To facilitate tidy and easy-to-follow code, we introduce a few important utility functions, which are put to use throughout the codebase.

### File paths

Given the reliance of files located in a variety of subdirectories that could be operating on multiple platforms, we constructed some simple functions for managing and point to particular folders or files.

* `file_ext` determines the file extension, based on the separating character (`sep_char`), which facilitates use with generalized URL APIs
* `path_no_ext` provides extension-removing services.
* `main_path`, `sub_paths`, `model_paths`, and `file_paths` all provide paths to the named components, with the `_paths` indicating that multiple paths can be obtained at once (not the case for the `main` directory, as there is only one).

### Data IO

*portalcasting* has a generalized `read_data` function that allows for toggling among `read_rodents`, `read_rodents_table`, `read_covariates`, `read_covariate_casts`, `read_moons`, and `read_metadata`, which each have specific loading procedures in place and, in the case that the requested file does not exist, an attempt is made (through the corresponding `prep_` function) to create the data file. Similar to the `read_data` functions, `read_casts` provides a simple user interface for reading the cast files into the R session.

For saving out, `data_out` provides a simple means for interfacing with potentially pre-existing data files, with logical inputs for saving generally and overwriting a pre-existing file specifically, and flexible file naming. The type of data saved out is currently restricted to `.csv` and `.yaml`, which is extracted from the filename given. 


### `check_args`

`check_args` checks the validity of inputted values for arguments, variably (depending on the argument) including the class, length, and values. It extracts the boolean checking code that would normally be within each function, placing it instead within `check_arg` which is called for each argument by `check_args`.

A key aspect to the *portalcasting* pipeline's ability to connect pieces is tied directly to argument checking: argument names should not be re-used in different functions unless they are for the same type of object. 

Some base *portalcasting* utilities cannot use `check_args` however, and so have their own specific internal checking. For example, `list_depth`'s recursive nature prevents its use, and `check_arg` cannot use `check_args` itself. 

`check_args` provides important backstopping and error checking of a standard *portalcasting* production pipepline. However, it is likely that a user in development spaces (a.k.a. sandboxes) would like to deviate from the standard behavior to leverage, for example, the ability to construct novel rodent `tmnt_types` (groups of observations over time). To that end, we include an argument `arg_checks` that is in nearly every function throughout the codebase, which flips a switch and turns off the vast majority of the *portalcasting*-generated errors. In this space, the user will be able to create a much broader array, but will likely be first informed of errors through more base functions. To that end, in default sandboxing mode, we set `arg_checks = FALSE` (minimal checking from *portalcasting*) and `verbose = TRUE` (maximum feedback from underlying functions).

### `messageq`

`messageq` provides a simple wrapper on `message` that also has a logical input for quieting. This helps switch messaging off as desired while localizing the actual boolean operator code to one spot.
